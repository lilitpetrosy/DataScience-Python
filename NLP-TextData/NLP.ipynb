{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Sentiment Analysis on IMDB Movie Reviews\n",
    "\n",
    "In this assignment we will be exploring tools for Natural Language Processing (NLP). Our task is sentiment analysis for movie reviews and in that context we will touch upon multiple areas:\n",
    "\n",
    "- Feature engineering\n",
    "- Bag of words modeling\n",
    "- Word2Vec modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (4.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /opt/conda/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/conda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import Beautiful Soup, NumPy and Pandas, etc\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    " \n",
    "# download NLTK classifiers - these are cached locally on your machine\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator\n",
    "\n",
    "# import word2vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "___\n",
    "\n",
    "### Data Description\n",
    ">Data source: https://www.kaggle.com/c/word2vec-nlp-tutorial/data (originally from [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/))<br>\n",
    ">\n",
    ">Data Description:<br><br>\n",
    ">We will be using Kaggle's **Bag of Words Meets Bags of Popcorn** dataset to explore [IMBD](https://www.imdb.com/) movie review data. This dataset in included with the zip file distribution of your homework. Labeled training dataset consists of 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews. The training data set is constructed in a balanced way so that there are an equal number of positive and negative reviews for each movie. There is also an unlabeled test set with 25,000 IMDB movie reviews. We don't use this for testing, but we do use it to improve unsupervised learning.\n",
    ">\n",
    ">Data Sets:<br>\n",
    ">* ```labeledTrainData.tsv``` --> The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id (numerical), sentiment (categorical), and text for each review (textual).<br>\n",
    ">* ```testData.tsv``` --> The unlabeled test set. 25,000 rows containing an id (numerical), and text for each review (textual). \n",
    ">\n",
    "> Further Reading:<br>\n",
    "> \n",
    "> * [Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Preparing data for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided the function `review_cleaner` to preprocess reviews. Here is an overview of what it does:\n",
    "\n",
    "> - Removes HTML tags (using beautifulsoup)\n",
    "> - Extract emoticons (emotion symbols, aka smileys :D )\n",
    "> - Removes non-letters (using regular expression)\n",
    "> - Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "> - Removes all the English stopwords from the list of movie review words\n",
    "> - Applies either stemming or lemmatization, as indicated by the arguments\n",
    "> - Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "Note that you do not need to make any changes to `review_cleaner`. We will explore some examples of the cleaning process below.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def review_cleaner(review, lemmatize=True, stem=False):\n",
    "    '''\n",
    "        Clean and preprocess a review.\n",
    "            1. Remove HTML tags\n",
    "            2. Extract emoticons\n",
    "            3. Use regex to remove all special characters (only keep letters)\n",
    "            4. Make strings to lower case and tokenize / word split reviews\n",
    "            5. Remove English stopwords\n",
    "            6. Lemmatize\n",
    "            7. Rejoin to one string\n",
    "        \n",
    "        @review (type:str) is an unprocessed review string\n",
    "        @return (type:str) is a 6-step preprocessed review string\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    if lemmatize == True and stem == True:\n",
    "        raise RuntimeError(\"May not pass both lemmatize and stem flags\")\n",
    "\n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review).text    \n",
    "\n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "\n",
    "    #3. Remove punctuation\n",
    "    review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review = review.lower().split()\n",
    "\n",
    "    #5. Remove stopwords, Lemmatize, Stem\n",
    "    clean_review=[]\n",
    "    for word in review:\n",
    "        if word not in eng_stopwords:\n",
    "            if lemmatize is True:\n",
    "                word=wnl.lemmatize(word)\n",
    "            elif stem is True:\n",
    "                if word == 'oed':\n",
    "                    continue\n",
    "                word=ps.stem(word)\n",
    "            clean_review.append(word)\n",
    "\n",
    "    #6. Join the review to one sentence\n",
    "    review_processed = ' '.join(clean_review+emoticons)\n",
    "    \n",
    "    return review_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To make things interesting, everyone gets to analyze a different review. Set `seed_value` to your favorite number, your name, or whatever else you'd like.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q0_set_seed\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "seed_value = \"lilit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q0_set_seed</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q0_set_seed passed!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q0_set_seed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My mother keeps a cassette of this film as a general threat to any film loving person who annoys her. Everything about it stinks.<br /><br />As such it is a true classic.<br /><br />Who gave it 10/10? Were you inadvertently watching a good film and accidentally voted for this one?<br /><br />Everyone involved in the movie making process should be forced to watch at least a small section of this film. It should be an indelible stain on the minds on all that hold film sacred and be revered as the tide mark of the cinematically dire.\"\n"
     ]
    }
   ],
   "source": [
    "# Print out a cleaned version of the randomly selected review\n",
    "my_review_id = int(hashlib.md5(str(seed_value).encode(\"utf-8\")).hexdigest()[:8], 16) % len(train.index)\n",
    "my_review = train.iloc[my_review_id][\"review\"]\n",
    "print(my_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 - Find the stopwords\n",
    "\n",
    "By manual inspection, find the first 5 stopwords in your chosen review. It might seem easier to write the code to do this, but the point of the exercise is to understand what the algorithm is doing.\n",
    "\n",
    "First review the list of stopwords below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you you're you've you'll you'd your yours yourself yourselves he him his himself she she's her hers herself it it's its itself they them their theirs themselves what which who whom this that that'll these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don don't should should've now d ll m o re ve y ain aren aren't couldn couldn't didn didn't doesn doesn't hadn hadn't hasn hasn't haven haven't isn isn't ma mightn mightn't mustn mustn't needn needn't shan shan't shouldn shouldn't wasn wasn't weren weren't won won't wouldn wouldn't\n"
     ]
    }
   ],
   "source": [
    "# See what the stopwords are\n",
    "print(\" \".join(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Inspect the review and look for the 5 first stopwords. Store them in `first_5_stopwords` in the order in which they appear in the review.\n",
    "\n",
    "e.g., \n",
    "```\n",
    "first_5_stopwords = ['having', 'the', 'to', 'some', 'of']\n",
    "```\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_stopwords_type\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "first_5_stopwords = ['my', 'a', 'of', 'this', 'as']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_stopwords_type</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_stopwords_type passed!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_stopwords_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_stopwords_length\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_stopwords_length</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_stopwords_length passed!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_stopwords_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_stopwords_match\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q1_stopwords_match</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q1_stopwords_match passed!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1_stopwords_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 - Lemmatization\n",
    "\n",
    "Lemmatization allows grouping of common forms of a word.\n",
    "\n",
    "Here are some examples of lemmatization:\n",
    "* images -> image\n",
    "* waxworks -> waxwork\n",
    "* sweets -> sweet\n",
    "\n",
    "By manual inspection, find the first 3 words in `my_review` that are lemmatized. Store them in `first_3_lemmatized` in the order in which they appear in the review.\n",
    "\n",
    "E.g.:\n",
    "```\n",
    "first_3_lemmatized = ['images', 'waxworks', 'sweets']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"My mother keeps a cassette of this film as a general threat to any film loving person who annoys her. Everything about it stinks.<br /><br />As such it is a true classic.<br /><br />Who gave it 10/10? Were you inadvertently watching a good film and accidentally voted for this one?<br /><br />Everyone involved in the movie making process should be forced to watch at least a small section of this film. It should be an indelible stain on the minds on all that hold film sacred and be revered as the tide mark of the cinematically dire.\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_lemmatization_type\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization examples:\n",
      "keeps -> keep\n",
      "stinks -> stink\n",
      "minds -> mind\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "first_3_lemmatized = ['keeps', 'stinks', 'minds']\n",
    "\n",
    "print(\"Lemmatization examples:\")\n",
    "for w in first_3_lemmatized:\n",
    "    print(\"{} -> {}\".format(w, wnl.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2_lemmatization_type</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2_lemmatization_type passed!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2_lemmatization_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_lemmatization_length\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2_lemmatization_length</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2_lemmatization_length passed!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2_lemmatization_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_lemmatization_match\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q2_lemmatization_match</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q2_lemmatization_match passed!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2_lemmatization_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3 - Stemming\n",
    "\n",
    "Lemmatization allows grouping of common forms of a word.\n",
    "\n",
    "Here are some examples of stemming:\n",
    "* nonsense -> nonsens\n",
    "* investigates -> investig\n",
    "* disappearance -> disappear\n",
    "\n",
    "By manual inspection, find the first 3 words in `my_review` that are modified by stemming. Store them in `first_3_stemmed` in the order in which they appear in the review.\n",
    "\n",
    "E.g.:\n",
    "```\n",
    "first_3_stemmed = ['nonsense', 'investigates', 'disappearance']\n",
    "```\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_stemming_type\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"My mother keeps a cassette of this film as a general threat to any film loving person who annoys her. Everything about it stinks.<br /><br />As such it is a true classic.<br /><br />Who gave it 10/10? Were you inadvertently watching a good film and accidentally voted for this one?<br /><br />Everyone involved in the movie making process should be forced to watch at least a small section of this film. It should be an indelible stain on the minds on all that hold film sacred and be revered as the tide mark of the cinematically dire.\"'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming examples:\n",
      "keeps -> keep\n",
      "cassette -> cassett\n",
      "general -> gener\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "first_3_stemmed = ['keeps', 'cassette', 'general']\n",
    "\n",
    "print(\"Stemming examples:\")\n",
    "for w in first_3_stemmed:\n",
    "    print(\"{} -> {}\".format(w, ps.stem(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q3_stemming_type</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q3_stemming_type passed!"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3_stemming_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_stemming_length\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q3_stemming_length</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q3_stemming_length passed!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3_stemming_length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3_stemming_match\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q3_stemming_match</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q3_stemming_match passed!"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3_stemming_match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "## Part II: Train and validate a sentiment analysis model using a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we have written the code to train the classifier for you. Your task will be to explore its performance characteristics with your own movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text using a bag of words model\n",
    "def get_vectorizer(ngram, max_features):\n",
    "    return CountVectorizer(ngram_range=(1, ngram),\n",
    "                             analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = review_cleaner,\n",
    "                             stop_words = None, \n",
    "                             max_features = max_features)\n",
    "\n",
    "# Model training\n",
    "def train_predict_sentiment(reviews, vectorizer, y=train[\"sentiment\"], ngram=1, max_features=1000, model_random_state=123):\n",
    "    '''\n",
    "        This function will:\n",
    "            1. split data into train and test set.\n",
    "            2. get n-gram counts from cleaned reviews \n",
    "            3. train a random forest model using train n-gram counts and y (labels)\n",
    "            4. test the model on your test split\n",
    "            5. print accuracy of sentiment prediction on test and training data\n",
    "            6. print confusion matrix on test data results\n",
    "\n",
    "            To change n-gram type, set value of ngram argument\n",
    "            To change the number of features you want the countvectorizer to generate, set the value of max_features argument\n",
    "            \n",
    "            @cleaned_review (type:str) is preprocessed string from review_cleaner()\n",
    "            @return none\n",
    "    '''\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarray() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train)\n",
    "    if not isinstance(train_bag, np.ndarray):\n",
    "        train_bag = train_bag.toarray()\n",
    "    test_bag = vectorizer.transform(X_test)\n",
    "    if not isinstance(test_bag, np.ndarray):\n",
    "        test_bag = test_bag.toarray()\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50, random_state = model_random_state) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    # validation\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    return forest\n",
    "\n",
    "# Print out the top features\n",
    "def top_features(forest, vectorizer, n):\n",
    "    #Extract feature importance\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    feature_text = vectorizer.get_feature_names().copy()\n",
    "    feature_importance = forest.feature_importances_.copy()\n",
    "    \n",
    "    indices = np.argsort(feature_importance)[::-1]\n",
    "    \n",
    "    top_n_ind = indices[:n]\n",
    "    top_n = list([vectorizer.get_feature_names()[ind] for ind in top_n_ind])\n",
    "    \n",
    "    print(top_n)\n",
    "\n",
    "# Print out whether the prediction is accurate\n",
    "def check_prediction(model, vectorizer, review, expected):\n",
    "    prediction = model.predict(vectorizer.transform([review]))[0]\n",
    "    sentiment = \"👍\" if prediction else \"👎\"\n",
    "    correct = \"\\x1b[92mcorrect\\x1b[0m\" if prediction == expected else \"\\x1b[31mincorrect\\x1b[0m\"\n",
    "    print(\"{} ⟶ {} {}\".format(review, sentiment, correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9999 \n",
      " The validation accuracy is:  0.7184\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [1853  695]\n",
      "     pos   [ 713 1739]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'great', 'movie', 'film', 'best', 'one', 'even', 'like', 'nothing', 'love', 'good', 'well', 'plot', 'story', 'time', 'would', 'also', 'life', 'acting', 'character']\n"
     ]
    }
   ],
   "source": [
    "# Train RFC model\n",
    "vectorizer = get_vectorizer(ngram=1, max_features=100)\n",
    "forest_model = train_predict_sentiment(reviews=train[\"review\"], vectorizer=vectorizer, y=train[\"sentiment\"])\n",
    "top_features(forest_model, vectorizer, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 4 - Construct a positive sentiment review\n",
    "\n",
    "Think of a movie that you like and write a review for it. Store as a string in `good_review`. If the model doesn't give a positive prediction for your review iterate on it until it does.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4_positive_review_type\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love 27 Dresses, it is such an exquisitely written romcom, I could watch it over and over. ⟶ 👍 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "good_review = \"I love 27 Dresses, it is such an exquisitely written romcom, I could watch it over and over.\"\n",
    "check_prediction(forest_model, vectorizer, good_review, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q4_positive_review_type</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q4_positive_review_type passed!"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4_positive_review_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4_positive_review_prediction\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q4_positive_review_prediction</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q4_positive_review_prediction passed!"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4_positive_review_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 5 - Construct a negative sentiment review\n",
    "\n",
    "Think of a movie that you like and write a review for it. Store as a string in `bad_review`. If the model doesn't give a negative prediction for your review iterate on it until it does.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_negative_review_type\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click is a scary and saddening movie. It made me cry, I do not recommend. ⟶ 👎 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "bad_review = \"Click is a scary and saddening movie. It made me cry, I do not recommend.\"\n",
    "check_prediction(forest_model, vectorizer, bad_review, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q5_negative_review_type</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q5_negative_review_type passed!"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5_negative_review_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_negative_review_prediction\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q5_negative_review_prediction</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q5_negative_review_prediction passed!"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5_negative_review_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 6 - Construct a misclassified negative sentiment review\n",
    "\n",
    "Now try to write a review that you view as negative but the model views as positive. Iterate and experiment as necessary and store it as a string  `bad_review_error`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6_misclassified_review_type\n",
    "manual: false\n",
    "points: 0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whoever created this movie must have been on crack. ⟶ 👍 \u001b[31mincorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "bad_review_error = \"Whoever created this movie must have been on crack.\"\n",
    "check_prediction(forest_model, vectorizer, bad_review_error, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q6_misclassified_review_type</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q6_misclassified_review_type passed!"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6_misclassified_review_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6_misclasified_review_prediction\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q6_misclasified_review_prediction</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q6_misclasified_review_prediction passed!"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6_misclasified_review_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=[utils.simple_preprocess(review) for review in train['review']], size=100, seed=123, workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 - Explain Word2Vec similarity on display below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actor', 0.7527598142623901),\n",
       " ('singer', 0.7031463384628296),\n",
       " ('performance', 0.6848963499069214),\n",
       " ('role', 0.6806384325027466),\n",
       " ('performer', 0.6665074825286865),\n",
       " ('comedienne', 0.6070108413696289),\n",
       " ('accent', 0.5900567770004272),\n",
       " ('dancer', 0.5877708792686462),\n",
       " ('emma', 0.580363929271698),\n",
       " ('lady', 0.5798507332801819)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['actress'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Pick one word similar to 'actress' and explain why it appears\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7_word2vec_similar_actress\n",
    "manual: true\n",
    "points: 1\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The word 'actor' is chosen as similar to 'actress' due to stemming, as the two words share the same stem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 - Explain Word2Vec comparison on display below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('herself', 0.443256139755249),\n",
       " ('queen', 0.4422002136707306),\n",
       " ('sabrina', 0.4362642168998718),\n",
       " ('her', 0.41051173210144043),\n",
       " ('woman', 0.4099072217941284),\n",
       " ('beautiful', 0.4021422564983368),\n",
       " ('victoria', 0.3905547261238098),\n",
       " ('stanwyck', 0.3861871659755707),\n",
       " ('whore', 0.37464654445648193),\n",
       " ('beauty', 0.3689585030078888)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=['actress'], negative=['actor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Pick one word similar to 'actress' and dissimilar to 'actor' explain why it appears\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8_word2vec_similar_actress_dissimilar_actor\n",
    "manual: true\n",
    "points: 1\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The word 'herself' is chosen as similar to 'actress' as actresses are female and thus use female pronouns. On the otherhad, 'actor' is associated with male pronouns and thus is not similar to 'herself'.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vecs(reviews, model):\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    reviewFeatureVecs = []\n",
    "    for counter, review in enumerate(reviews):\n",
    "        if (counter + 1) % 5000. == 0.:\n",
    "            print(\"Review %d of %d\" % (counter + 1, len(reviews)))\n",
    "        featureVec = []\n",
    "        for n,word in enumerate(utils.simple_preprocess(review)):\n",
    "            if word in index2word_set: \n",
    "                featureVec.append(model.wv[word])\n",
    "        featureVec = np.mean(featureVec, axis=0).reshape(1,-1)\n",
    "\n",
    "        reviewFeatureVecs.append(featureVec)\n",
    "\n",
    "    return np.concatenate(reviewFeatureVecs, axis=0)\n",
    "\n",
    "w2v_vectorizer = FunctionTransformer(lambda x: get_avg_feature_vecs(x, w2v_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Review 5000 of 20000\n",
      "Review 10000 of 20000\n",
      "Review 15000 of 20000\n",
      "Review 20000 of 20000\n",
      "Review 5000 of 5000\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8048\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2049  499]\n",
      "     pos   [ 477 1975]\n"
     ]
    }
   ],
   "source": [
    "v2v_forest_model = train_predict_sentiment(reviews=train[\"review\"], vectorizer=w2v_vectorizer, y=train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 9 - compare Word2Vec to Bag of Words\n",
    "\n",
    "Comment on how Word2Vec compares with the Bag of Words Model. Please use the template below for your answer\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q9_word2vec_comparison\n",
    "manual: true\n",
    "points: 2\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete answers below*:\n",
    "\n",
    "* **Is it an improvement?**\n",
    "\n",
    "*The Word2Vec model is an improvement, as it produces a higher training accuracy (1.0 vs. 0.9999) and validation accuracy (0.8048 vs. 0.7184) than the Bag of Words model.*\n",
    "\n",
    "* **How significant is the difference?**\n",
    "\n",
    "*The difference in validation accuracy is significant, as it is 10% higher for the Word2Vec model. Thus with larger datasets, the 10% difference can produce a large discrepancy in the magnitude of reviews inaccurately classified.*\n",
    "\n",
    "* **Is this a fair and meaningful comparision? Why or why not?**\n",
    "\n",
    "*This comparison is not fair/meaningful as it is based on a single dataset that may contain baised or inaccurate data.*\n",
    "\n",
    "* **What other experiments might you run to further compare?**\n",
    "\n",
    "*I would experiment by either training the two models with 1) smaller/varied samples from the dataset or 2) different datasets. After such, I would compare the two models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Add more training data\n",
    "\n",
    "You will now try to further improve the performance of the Word2Vec model by enhancing it with unlabeled data. \n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q10_word2vec_train_more\n",
    "manual: true\n",
    "points: 2\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unlabeled test data set\n",
    "more_training_data = pd.read_csv(\"testData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first 5 rows\n",
    "more_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 40)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.build_vocab(more_training_data, update = True)\n",
    "w2v_model.train(more_training_data, total_examples = w2v_model.corpus_count, epochs = w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Review 5000 of 20000\n",
      "Review 10000 of 20000\n",
      "Review 15000 of 20000\n",
      "Review 20000 of 20000\n",
      "Review 5000 of 5000\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.8048\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2049  499]\n",
      "     pos   [ 477 1975]\n"
     ]
    }
   ],
   "source": [
    "w2v_forest_model = train_predict_sentiment(reviews=train[\"review\"], vectorizer=FunctionTransformer(lambda x: get_avg_feature_vecs(x, w2v_model)), y=train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 - Comment on the impact of more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete answers below*:\n",
    "\n",
    "* **Did adding more training data improve the model?**\n",
    "\n",
    "*No, the models are the same.*\n",
    "\n",
    "* **How significant is the difference?**\n",
    "\n",
    "*There is no difference.*\n",
    "\n",
    "* **Why could one expect a model to improve even when provided with unlabeled data?**\n",
    "\n",
    "*The model could improve as it is assumed the labeled and unlabeled data come from the same distribution, thus increasing the number of data points on which the model is trained, increasing accuracy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Prediction Analysis\n",
    "\n",
    "Check to see how the Word2Vec model works on the reviews that you wrote previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love 27 Dresses, it is such an exquisitely written romcom, I could watch it over and over. ⟶ 👍 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_prediction(w2v_forest_model, w2v_vectorizer, good_review, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click is a scary and saddening movie. It made me cry, I do not recommend. ⟶ 👍 \u001b[31mincorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "check_prediction(v2v_forest_model, w2v_vectorizer, bad_review, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bag of Words:\n",
      "Whoever created this movie must have been on crack. ⟶ 👍 \u001b[31mincorrect\u001b[0m\n",
      "With Word2Vec:\n",
      "Whoever created this movie must have been on crack. ⟶ 👎 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"With Bag of Words:\")\n",
    "check_prediction(forest_model, vectorizer, bad_review_error, 0)\n",
    "\n",
    "print(\"With Word2Vec:\")\n",
    "check_prediction(v2v_forest_model, w2v_vectorizer, bad_review_error, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 11 - how does the Word2Vec model compare to Bag of Words?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q11_word2vec_comment\n",
    "manual: true\n",
    "points: 1\n",
    "-->\n",
    "\n",
    "Just comment, don't change your reviews to achieve a particular outcome.\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Complete answers below:*\n",
    "\n",
    "* **Is your positive review classified correctly by Word2Vec?**\n",
    "\n",
    "*Yes, my positive review is correctly classified as positive by Word2Vec.*\n",
    "\n",
    "* **Is your negative review classified correctly by Word2Vec?**\n",
    "\n",
    "*No, my negative review is incorrectly classified as positive by Word2Vec.*\n",
    "\n",
    "* **Is your negative review misclassified by Bag of Words now classified correctly by Word2Vec?**\n",
    "\n",
    "*Yes, my negative review misclassified by Bag of Words is now classified correctly by Word2Vec.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 12 - create a review where the models disagree\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q12_word2vec_split_decision_exists\n",
    "manual: false\n",
    "points: 0\n",
    "-->\n",
    "\n",
    "If your originally misclassified negative review was properly classified by Word2Vec you may use it to answer this question.\n",
    "If not, construct some other review that is properly classified by one model and improperly classified by the other model. Store that review as a string in `split_prediction`. Store the expected prediction in `split_prediction_expected` as 1 for positive sentiment or 0 for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bag of Words:\n",
      "Whoever created this movie must have been on crack. ⟶ 👍 \u001b[31mincorrect\u001b[0m\n",
      "With Word2Vec:\n",
      "Whoever created this movie must have been on crack. ⟶ 👎 \u001b[92mcorrect\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "split_prediction = \"Whoever created this movie must have been on crack.\"\n",
    "split_prediction_expected = 0\n",
    "\n",
    "print(\"With Bag of Words:\")\n",
    "check_prediction(forest_model, vectorizer, split_prediction, split_prediction_expected)\n",
    "\n",
    "print(\"With Word2Vec:\")\n",
    "check_prediction(v2v_forest_model, w2v_vectorizer, split_prediction, split_prediction_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q12_word2vec_split_decision_defined\n",
    "manual: true\n",
    "points: 1\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q12_word2vec_split_decision_defined</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q12_word2vec_split_decision_defined passed!"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q12_word2vec_split_decision_defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q12_word2vec_split_decision_predict\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <p><strong>q12_word2vec_split_decision_predict</strong> passed!</p>\n",
       "    "
      ],
      "text/plain": [
       "q12_word2vec_split_decision_predict passed!"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q12_word2vec_split_decision_predict\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
